\noindent Given a reasonable size of our problem, the corresponding
MMDP model can be very large. For example, with 8 players and 17
tasks in a 50$\times$55 grid, the number of possible states is more
than $2\times 10^{400}$. Therefore, it is computationally
intractable to compute the optimal solution. One useful observation
of our problem is: when making a decision, the responders first
need to {\em cooperatively} select a task to form a team with
others. Then they can {\em independently} compute the best path to
the task. In our planning algorithm, we use this observation to
decompose the decision-making process into a hierarchical structure
with two levels:
\begin{itemize}
  \item In the higher level, task planning algorithm is run for
      the whole team to assign the best task to each responders
      given the current state.
  \item In the lower level, by given a task, path planning
      algorithm is run for each responder to find the best path
      to the task from his or her current location.
\end{itemize}

Furthermore, not all states are relevant to the problem (e.g., if a
responder gets injured, he or she is incapable to do any task in
the future and therefore his or her states are irrelevant to other
responders) and we only need to consider the reachable states given
the current state of the problem. Hence, given the current state,
we compute the policy online only for reachable states. This saves
a lot of computation because the size of the reachable states is
usually much smaller than the overall state space. Another
advantage of online planning is that it allows us to tweak the
model as more information is obtained or unexpected events happen.
For example, if the wind becomes stronger, the uncertainty about
the radioactive cloud may increase. If a responder becomes tired,
his or her actions can be less reliable.

The main process of our online hierarchical planning algorithm is
outlined in Algorithm~\ref{alg:coordination}. The following
sections will describe the procedures of each level in more detail.

\begin{algorithm}[t]
  \caption{Team Coordination}
  \KwIn{the MMDP model and the current state $s$.}
  \KwOut{the best joint action $\vec{a}$.}
  \tcp{The task planning}
  $\{ t_i \} \gets$ compute the best task for each responder $i\in I$ \;
  \ForEach{$i\in I$} {
    \tcp{The path planning}
    $a_i \gets$ compute the best path to task $t_i$ \;
  }
  \Return{$\vec{a}$}
  \label{alg:coordination}
\end{algorithm}

\subsection{Task planning}
\label{sec:taskplanning}

\noindent As aforementioned, each responder in our problem has a
specific role to determine which task he or she can perform. A task
can only be completed by a team of responders with the required
roles. Thus, the goal of task planning is to assign a task to each
responder that maximises the team performance given the current
state $s$. To this end, we first compute all possible coalitions
$\{ C_{jk} \}$ for each task $t_j$ where a coalition $C_{jk}
\subseteq I$ is a group of the responders with the required roles.
Apparently, if a task has been completed, we do not need to
consider it any more. If a responder is incapable of performing the
task, he or she will be removed from the coalitions. This
information can be obtained from the state $s$. Because the role of
each responder and the requirement of each task is static, we can
compute all possible coalitions offline. During the online phase,
we only need to filter out the coalitions for completed tasks or
with incapable responders to compute the coalition set $\{ C_{jk}
\}$.

Given the coalition set computed above, we then solve the following
optimisation problem to find the best solution:
\begin{equation}
  \begin{array}{lll}
    \max\limits_{x_{jk}} & \sum_{j, k} x_{jk} \cdot v(C_{jk}) & \\[2pt]
    \mbox{s.t.} & x_{jk} \in \{0, 1\} & \\[2pt]
    & \forall j, \sum_{k} x_{jk} \leq 1 & \mbox{(i)} \\[2pt]
    & \forall i, \sum_{j, k} \delta_i(C_{jk}) \leq 1 & \mbox{(ii)}
  \end{array}
  \label{eq:cf}
\end{equation}
where $x_{jk}$ is the boolean variable to indicate whether
coalition $C_{jk}$ is selected for task $t_j$ or not, $v(C_{jk})$
is the characteristic function for coalition $C_{jk}$, and
$\delta_i(C_{jk}) = 1$ if responder $p_i\in C_{jk}$ and 0
otherwise. In the optimisation, Constraint (i) ensures that a task
$j$ is allocated at most to only one coalition (a task does not
need more than one group of responders). Constraint (ii) ensures
that a responder $i$ is assign to only one task (a responder cannot
do more than one task at the same time). This is a standard MILP
that can be efficiently solved by CPLEX.

In order to solve Equation~\ref{eq:cf}, we need to compute the
value of $v(C_{jk})$ for each coalition $C_{jk}$, which is the
long-term value when the responders in $C_{jk}$ are assigned to
task $t_j$. This is challenging because not all tasks can be
completed in one shot and the policy after completing task $t_j$
must be computed as well, which is time-consuming. Alternatively,
we can estimate the value by several simulations. This is much
cheaper because we do not need to compute the complete policy.
According to the central limit theorem, as long as the number of
simulations are sufficient large, the estimated value will converge
to the true coalition value. The main process is outlined in
Algorithm~\ref{alg:tp}.

\begin{algorithm}[t]
  \caption{Task Planning}
  \KwIn{the current state $s$,
  a set of unfinished tasks $T$,
  and a set of free responders $I$.}
  \KwOut{a task assignment for all responders.}
  $\{ C_{jk} \} \gets$ compute all possible coalitions of $I$ for
  $T$ \;
  \ForEach{$C_{jk} \in \{C_{jk}\}$}{
    \tcp{The $N$ trial simulations}
    \For{$i=1$ \KwTo $N$}{
        $(r, s') \gets$ simulate the process with the starting \\\Indp state $s$
        until task $k$ is completed by the responders in $C_{jk}$ \; \Indm
        \If{$s'$ is a terminal state} {
            $v_i(C_{jk}) \gets r$ \;
        } \Else {
            $V(s') \gets$ estimate the value of $s'$ with MCTS \;
            $v_i(C_{jk}) \gets r + \gamma V(s')$ \;
        }
    }
    $v(C_{jk}) \gets \frac{1}{N} \sum_{i=1}^{N} v_i(C_{jk})$ \;
  }
  \Return the task assignment computed by Equation~\ref{eq:cf}
  \label{alg:tp}
\end{algorithm}

In each simulation, we first assign the responders in $C_{jk}$ to
task $t_j$ and run the simulator starting from the current state
$s$. After task $t_j$ is completed, the simulator returns the sum
of the rewards $r$ and the new state $s'$. If all the responders in
$C_{jk}$ are incapable to do other tasks (e.g., receiving too many
radioactive doses), the simulation is terminated. Otherwise, we
estimate the expected value of $s'$ using Monte-Carlo Tree Search
(MCTS), which provides good tradeoff between exploitation and
exploration of the policy space and has been shown to be efficient
for large MDPs~\cite{?}. The basic idea of MCTS is to maintain a
search tree where each node is associated with a state $s$ and each
branch is a task assignment for all responders. After $N$
simulations, the averaged value is returned as an approximation of
the coalition value.

In the task planning level, ``completing a task by a responder'' is
a macro action, assuming that each responder can find the best path
to the task (Section~\ref{sec:pathplanning} gives more detail about
how to compute this). Thus, the main step of implementing MCTS is
to compute an assignment for the free responders (A responder is
free when he or she is capable of doing tasks but not assigned to
any task) at each node of the search tree. This can be computed by
Equation~\ref{eq:cf} using the coalition values estimated by the
UCT heuristic~\cite{?}:
\begin{equation}
  v(C_{jk}) = \overline{v(C_{jk})} + c\sqrt{\frac{2N(s)}{N(s, C_{jk})}}
\end{equation}
where $\overline{v(C_{jk})}$ is the averaged value of coalition
$C_{jk}$ at state $s$ so far, $c$ is a tradeoff constant, $N(s)$ is
the visiting frequency of state $s$, and $N(s, C_{jk})$ is the
frequency that coalition $C_{jk}$ has been selected at state $s$.
Intuitively, if a coalition $C_{jk}$ has bigger averaged value
$\overline{v(C_{jk})}$ or is rarely selected ($N(s, C_{jk})$ is
smaller), it has higher chance to be selected in the next visit of
the tree node.

Once the value of every coalition in $\{ C_{jk} \}$ has been
computed, we solve Equation~\ref{eq:cf} and return the best
assignment of the tasks. One main advantage of our approach is that
it can straightforwardly incorporate the preferences of the
responders. For example, if a responder rejects to do a task, we
simply filter out the coalitions for the task that contain the
responder. By so doing, the responder will not be assigned to the
task. Moreover, if a responder prefers doing tasks with another
responder, we can raise the weights of the coalitions that contain
them in Equation~\ref{eq:cf} (By default, all coalitions have
identical weights of 1.0). Thus, our approach is adaptive to
various preferences of human responders.

\subsection{Path planning}
\label{sec:pathplanning}

\noindent In the path planning, we compute the best path for a
responder given a task assigned to him or her. This path planning
is stochastic as there are uncertainties in the radioactive cloud
and the responders' actions. We model this problem as a
single-agent MDP that can be defined as a tuple, $\mathcal{M}_i =
\langle S_i, A_i, P_i, R_i \rangle$, where:
\begin{itemize}
  \item $S_i = S_r \times S_{p_i}$ is the state space. In this
      level, responder $p_i$ only need to consider the states
      of the radioactive cloud $S_r$ and his or her own states
      $S_{p_i}$ in the MMDP.
  \item $A_i$ is the set of $p_i$'s actions. In this level,
      responder $p_i$ only need to consider his or her moving
      actions.
  \item $P_i = P_r \times P_{p_i}$ is the transition function.
      In this level, responder $p_i$ only need to consider the
      spreading of the radioactive cloud $P_r$ and the changes
      of his or her locations and health levels when moving in
      the filed $P_{p_i}$, which are defined earlier in the
      MMDP.
  \item $R_i$ is the reward function. In this level, responder
      $p_i$ only need to consider the cost of movement and the
      penalty of receiving too many radioactive doses.
\end{itemize}

This is a typical MDP that can be solved by many state-of-the-art
MDP solvers~\cite{?}. Among them, we adopt Real-Time Dynamic
Programming (RTDP)~\cite{?} because it is very efficient for our
problem, a goal-directed MDP with large number of states. Instead
of exploring the whole state space, RTDP only visits the states
that are reachable from the initial state $s^0$ (the start location
of the responder). The main process is outline in
Algorithm~\ref{alg:pp}. If the goal is not reached in a number of
iterations, we assume there is not path between the start location
of the responder and the task location (either there are obstacles
on the path or the responder will be killed by the radioactivity on
the road).

\begin{algorithm}[t]
  \caption{Path Planning}
  \KwIn{the starting state $s^0$ and the goal state $s^g$.}
  \KwOut{a path from the starting location to the goal.}
  $s \gets s^0$ \;
  \Repeat{$s = s^g$}{
    \ForEach{$a\in A_i$}{
        $Q(s, a) \gets R_i(s, a) + \sum_{s'\in S_i} P_i(s'|s, a)
        V(s')$ \;
    }
    $a \gets \arg\max_{a'\in A_i} Q(s, a')$ \;
    $V(s) \gets Q(s, a)$ \;
    $s' \sim P_i(s'|s, a)$ \;
    $s \gets s'$ \;
  }
  \Return{$Q$}
  \label{alg:pp}
\end{algorithm}

There are several techniques we used to speed up the convergency of
RTDP. In our problem, the terrain of the field is static. Thus, we
can initialize the value function $V(s)$ using the cost map
computed offline without considering the radioactive cloud. The
cost map stores the shortest path and the cost value between any
two points in the map. This will help RTDP quickly navigate among
the obstacles (e.g., buildings, water pools, blocked roads) without
getting trapped in dead ends during the search. Another technique
is: when traversing the reachable states (i.e., $s'\in S_i$ in
Algorithm~\ref{alg:pp}), we only consider the responder's current
location and the neighboring points since $P_i(s'|s,a) = 0$ for
other points. This will further speed up the algorithm where the
main bottleneck is the huge state space.
