\subsection{The Optimisation Problem}
\noindent A Multi-agent Markov Decision Process (MMDP) is formally
defined as a tuple $\mathcal{M} = \langle I, S, \{A_i\}, P, R
\rangle$, where:
\begin{itemize}
  \item $I$ is a set of $n$ field responders and each responder
      is associated with a unique identifier number $p_i\in I$.
  \item $S = S_r \times S_{p_1} \times \cdots \times S_{p_n}
      \times S_{t_1} \times \cdots \times S_{t_m}$ is the state
      space. $S_r = \{l_{(x,y)}| (x, y) \in G\}$ is the state
      variable of the radioactive cloud to specify the
      radioactive level $l_{(x,y)}\in[0, 100]$ at every point
      $(x, y)\in G$. $S_{p_i} = \langle h_i, (x_i, y_i), t_j
      \rangle$ is the state variable for each responder $p_i$
      to specify his or her health level $h_i\in[0, 100]$, the
      coordinate $(x_i, y_j)$, and the task $t_j$ carried by
      the responder. $S_{t_j} = \langle st_j, (x_j, y_j)
      \rangle$ is the state variable for task $t_j$ to specify
      its status $st_j$ (picked up, dropped off, or idle) and
      coordinate $(x_j, y_j)$.
  \item $A_i$ is a set of responder $p_i$'s actions. Each
      responder can {\em stay} in the current location $(x_i,
      y_i)$, {\em move} to the 8 neighbouring locations, or
      {\em complete} a task located in $(x_i, y_i)$. A joint
      action $\vec{a}=\langle a_1, \cdots, a_n \rangle$ is a
      set of actions where $a_i\in A_i$, one for each
      responder.
  \item $P = P_r \times P_{p_1} \times P_{p_n} \times P_{t_1}
      \times P_{t_n}$ is the transition function.
      $P_r(s'_r|s_r)$ is the probability for the radioactive
      cloud to spread from state $s_r$ to $s'_r$. It caputers
      the uncertainty of the next radioactive levels of the
      environment due to the noisy sensor reading and the
      variation in wind speed and direction.
      $P_{p_i}(s'_{p_i}|s, a_i)$ is the probability for
      responder $p_i$ to transit to a new state $s'_{p_i}$ when
      executing action $a_i$. For example, when a responder is
      asked to go to a new location, he or she may not be there
      because he or she becomes tired, gets injured, or
      receives radiation doses that are life threatening.
      $P_{t_j}(s'_{t_j}|s, \vec{a})$ is the probability for
      task $t_j$. A task $t_j$ can only be completed by a team
      of responders with required roles locating in the same
      coordinate as $t_j$.
  \item $R$ is the reward function. If a task is completed, the
      team will be rewarded. There will be a penalty for the
      team if any responder gets injured or receives too many
      radioactive doses. Each action of the responders has a
      cost since it will consume energy of the responders.
\end{itemize}
A policy for the MMDP is a mapping from states to joint actions,
$\pi: S \rightarrow \vec{A}$ so that the responders know which
actions to take given the current state of the problem. The quality
of a policy $\pi$ is usually measured by its expected value
$V^\pi$, which can be computed recursively by the Bellman equation:
\begin{equation}
  V^\pi(s) = R(s, \pi(s)) + \gamma\sum_{s'\in S} P(s'|s, \pi(s)) V^\pi(s')
\end{equation}
where $\pi(s)$ is a joint action given $s$ and $\gamma\in(0, 1]$ is
the discounted factor. The goal of solving the MMDP is to find an
optimal policy $\pi^*$ that maximises the expected value with the
initial state $s^0$, $\pi^* = \arg\max_{\pi} V^\pi(s^0)$.

At each decision step, we assume the planning agent can fully
observe the state of the environment $s$ by collecting sensor
reading of the radioactive cloud, GPS data of the responders, etc.
Given a policy $\pi$ of the MMDP, a joint action $\vec{a}=\pi(s)$
can be selected and broadcasted to the responders (as mentioned
earlier). By so doing, each responder can be instructed by the
agent and know how to act in the field.
