\subsection{The Optimisation Problem}
\noindent Here we formalise the optimisation problem that needs to
be solved to coordinate the responders optimally. Hence, we define
this problem as a  Multi-agent Markov Decision Process (MMDP)
formally represented by tuple $\mathcal{M} = \langle I, S, \{A_i\},
P, R \rangle$, where $I$ is the set of actors as defined in the
previous section,  $S$ is the state space, $A_i$ is a set of
responder $p_i$'s actions, $P$ is the transition function, and $R£$
is the reward function. We elaborate on each of these below.

More specifically, $S= S_r \times S_{p_1} \times \cdots \times
S_{p_n} \times S_{t_1} \times \cdots \times S_{t_m}$ where $S_r =
\{l_{(x,y)}| (x, y) \in G\}$ is the state variable of the
radioactive cloud to specify the radioactive level $l_{(x,y)}\in[0,
100]$ at every point $(x, y)\in G$. $S_{p_i} = \langle h_i, (x_i,
y_i), t_j \rangle$ is the state variable for each responder $p_i$
to specify his or her health level $h_i\in[0, 100]$, the coordinate
$(x_i, y_j)$, and the task $t_j$ carried by the responder. $S_{t_j}
= \langle st_j, (x_j, y_j) \rangle$ is then the state variable for
task $t_j$ to specify its status $st_j$ (picked up, dropped off, or
idle) and coordinate $(x_j, y_j)$.

The three types of actions  (in set $A_i$) a responder can take
are: (i) {\em stay} in the current location $(x_i, y_i)$, (ii) {\em
move} to the 8 neighbouring locations, or (iii) {\em complete} a
task located in $(x_i, y_i)$. A joint action $\vec{a}=\langle a_1,
\cdots, a_n \rangle$ is a set of actions where $a_i\in A_i$, one
for each responder.

The transition function $P$ is defined in more detail as: $P= P_r
\times P_{p_1} \times P_{p_n} \times P_{t_1} \times P_{t_n}$ where:
\begin{itemize}
    \itemsep=-2pt
    \item $P_r(s'_r|s_r)$ is the probability for the
        radioactive cloud to spread from state $s_r$ to $s'_r$.
        It captures the uncertainty of the next radioactive
        levels of the environment due to the noisy sensor
        reading and the variation in wind speed and direction.
    \item $P_{p_i}(s'_{p_i}|s, a_i)$ is the probability for
        responder $p_i$ to transit to a new state $s'_{p_i}$
        when executing action $a_i$. For example, when a
        responder is asked to go to a new location, he or she
        may not be there because he or she becomes tired, gets
        injured, or receives radiation doses that are life
        threatening.
    \item $P_{t_j}(s'_{t_j}|s, \vec{a})$ is the probability for
        task $t_j$. A task $t_j$ can only be completed by a
        team of responders with required roles locating in the
        same coordinate as $t_j$.
\end{itemize}

Now,  if a task $t_j$ is completed (i.e., in $s_{t_j}$, the status
$st_j$ is marked as ``dropped off'' and the coordinate $(x_j, y_j)$
is within a drop off zone), the team will be rewarded using
function $R$. There will be a penalty for the team if a responder
$p_i$ gets injured or receives a high dose of radiation (i.e., in
$s_{p_i}$, the health level $h_i$ is 0). Moreover, we attribute a
cost to each of the responders' since it will requires them to
exert some effort (e.g., running or carrying objects).


Give the above definitions, a policy for the MMDP is a mapping from
states to joint actions, $\pi: S \rightarrow \vec{A}$ so that the
responders know which actions to take given the current state of
the problem. The quality of a policy $\pi$ is usually measured by
its expected value $V^\pi$, which can be computed recursively by
the Bellman equation:
\begin{equation}
  V^\pi(s) = R(s, \pi(s)) + \gamma\sum_{s'\in S} P(s'|s, \pi(s)) V^\pi(s')
\end{equation}
where $\pi(s)$ is a joint action given $s$ and $\gamma\in(0, 1]$ is
the discounted factor. The goal of solving the MMDP is to find an
optimal policy $\pi^*$ that maximises the expected value with the
initial state $s^0$, $\pi^* = \arg\max_{\pi} V^\pi(s^0)$.

At each decision step, we assume the planning agent can fully
observe the state of the environment $s$ by collecting sensor
readings of the radioactive cloud and GPS locations of the
responders. Given a policy $\pi$ of the MMDP, a joint action
$\vec{a}=\pi(s)$ can be selected and broadcast to the responders
(as mentioned earlier). By so doing, each responder can be
instructed by the agent and know how to act in the field. In the
next section we discuss the computational challenges of finding an
optimal policy and propose a scalable approximation algorithm for
this purpose.
