\section{Team Coordination Algorithm}
\label{sec:algo}

Although the MMDP model can capture the uncertainties of our
problem, it results in a very large search space. Therefore, it is
practically impossible to compute the optimal solution. In such
cases, we need to consider approximate solutions that result in
high quality allocations. To this end, we decompose the overall
decision-making process into a hierarchical structure with two
levels: at the top level, a {\em task planning} algorithm is run
for the whole team to assign the best task to each responder given
the current state of the world; at the lower level, given a task, a
{\em path planning} algorithm is run by each responder to find the
best path to the task from her current location. Furthermore,
because not all states of MMDPs are relevant to the problem, we
only need to consider the reachable states given the current state.
Hence, we compute the policy online starting from the current state
of the problem. This saves a considerable amount of computation
because the size of the reachable states is usually much smaller
than the overall state space. The following sections describe each
level of our {\em online hierarchical planning} algorithm in more
detail.

\subsection{Task Planning}
\label{sec:taskplanning}

As described earlier, each responder $p_i$ is of a specific type
$\theta_i \in \Theta$ that determines which task she can perform
and  a task $t$ can only be completed by a team of responders with
the required types $\Theta_t$. If, at some point in the execution
of a plan, a responder $p_i$ is incapable of performing a task
(e.g., because she is tired or suffered a high radiation dose), she
will be removed from the set of responders under consideration
(that is $I \to I \setminus p_i$). This information can be obtained
from the state $s \in S$. When a task is completed by a chosen
team, the task is simply removed from the set (that is $T \to
T\setminus t_k$ if $t_k$ has been completed).

Now, to capture the efficiency of groupings of responders at
performing tasks, we define the value of a team $v(C_{jk})$ that
reflects the level of performance of team $C_k$ in performing task
$t_j$. This is computed from the estimated rewards that the team
obtains for performing $t_j$.  Then, the goal of the task planning
algorithm is to assign a task to each team that maximises the
overall team performance given the current state $s$, i.e.,
$\sum_{j=1}^m v(C_{j})$ where $C_j$ is a team for task $t_j$ and
$\{ C_1, \cdots, C_m \}$ is a {\em partition} of $I$ ($\forall
j\neq j', C_j \bigcap C_{j'} = \emptyset$ and $\bigcup_{j=1}^m
C_j=I$). In what follows, we first detail the procedure to compute
the value of all teams that are valid in a given state and then
proceed to detail the main algorithm to allocate tasks.


\subsubsection{Team Value Calculation}

The computation of  $v(C_{jk})$ for each team $C_{jk}$ is
challenging because not all tasks can be completed by one
allocation (there are usually more tasks than responders).
Moreover, the policy after completing task $t_j$ must also be
computed by the agent, which is time-consuming given the number of
states and joint actions. Given this, we propose to estimate
$v(C_{jk})$ through several simulations. This is much cheaper
computationally as it avoids computing the complete policy to come
up with a good estimate of the team value, though we may not be
able to evaluate all possible future outcomes. According to the
central limit theorem, if the number of simulations is sufficiently
large, the estimated value will converge to the true $v(C_{jk})$.

Specifically, in each simulation, we first assign the responders in
$C_{jk}$ to task $t_j$ and run the simulator starting from the
current state $s$. After task $t_j$ is completed, the simulator
returns the sum of the rewards $r$ and the new state $s'$. If all
the responders in $C_{jk}$ are incapable of doing other tasks
(e.g., suffered radiation burns), the simulation is terminated.
Otherwise, we estimate the expected value of $s'$ using Monte-Carlo
Tree Search (MCTS)~\cite{kocsis2006bandit}, which provides a good
trade-off between exploitation and exploration of the policy space
and has been shown to be efficient for large MDPs.\footnote{Other
methods such as sequential greedy assignment or swap-based hill
climbing~\cite{proper2009solving} may also be useful. However, they
do not explore the policy space as well as MCTS.} After $N$
simulations, the average value is returned as an approximation of
the team value.

\subsubsection{Coordinated Task Allocation}
Given the team values computed above, we then solve the following
optimisation problem to find the best solution:
\begin{equation}
  \begin{array}{lll}
    \max\limits_{x_{jk}} & \sum_{j, k} x_{jk} \cdot v(C_{jk}) & \\[2pt]
    \mbox{s.t.} & x_{jk} \in \{0, 1\} & \\[2pt]
    & \forall j, \sum_{k} x_{jk} \leq 1 & \mbox{(i)} \\[2pt]
    & \forall i, \sum_{j, k} \delta_i(C_{jk}) \leq 1 & \mbox{(ii)}
  \end{array}
  \label{eq:cf}
\end{equation}
where $x_{jk}$ is the boolean variable to indicate whether team
$C_{jk}$ is selected for task $t_j$ or not, $v(C_{jk})$ is the
value of team $C_{jk}$, and $\delta_i(C_{jk}) = 1$ if responder
$p_i\in C_{jk}$ and 0 otherwise. In the optimisation, constraint
(i) ensures that a task $t_j$ is allocated to at most one team (a
task does not need more than one group of responders) and
constraint (ii) ensures that a responder $p_i$ is assigned to only
one task (a responder cannot do more than one task at the same
time). This is a standard {\em mixed integer linear program} that
can be efficiently solved using solvers (e.g., CPLEX or LP\_SOLVE).

\subsubsection{Adapting to Responder Requests}\label{sec:adaptive}
An important characteristic of our approach is that it can easily
incorporate the preferences of the responders. For example, if a
responder declines a task allocated to it by the planning agent, we
simply filter out the teams for the task that contain this
responder. By so doing, the responder will not be assigned to the
task. Moreover, if a responder prefers to do the tasks with another
responder, we can increase the weights of the teams that contain
them in Equation~\ref{eq:cf} (by default, all teams have identical
weights of 1.0). Thus, our approach is adaptive to the
 preferences of human responders.

\subsection{Path Planning}
\label{sec:pathplanning}

In the path planning phase, we compute the best path for a
responder to her assigned task. This phase is stochastic as there
are uncertainties in the radioactive cloud and the responders'
actions. We model this problem as a single-agent MDP that can be
solved by many existing solvers. Among them, we choose {\em
real-time dynamic programming} (RTDP)~\cite{barto1995learning}
because it is simple and particularly fits our problem, that is, a
goal-directed MDP with large number of states. However, other
approaches for solving large MDPs  could equally be used here.

\subsection{Simulation Results}

Before deploying our solution (as part of $PA$) to advise human
responders, it is important  to test its performance to ensure it
can return efficient solutions on simulations of the real-world
problem. Given there is no extant solution that takes into account
uncertainty in team coordination for emergency response, we compare
our algorithm with a greedy and a myopic method to evaluate the
benefits of coordination and lookahead. For each method, we use our
path planning algorithm to compute the path for each responder. In
the greedy method, the responders are uncoordinated and select the
closest tasks they can do. In the myopic method, the responders are
coordinated to select the tasks but have no lookahead for the
future tasks.

Table~\ref{tab:simulation} shows the results for a problem with 17
tasks and 8 responders on a 50$\times$55 grid. As can be seen, our
MMDP algorithm completes more tasks than the myopic and greedy
methods (see Table \ref{tab:simulation}). More importantly, our
algorithm guarantees the safety of the responders, while in the
myopic method  only 25\% of the responders survive and in the
greedy method all responders are killed by the radioactive cloud.
More extensive evaluations are beyond the scope of this paper as
our focus here is on the use of the algorithm in a field deployment
to test how humans take up advice computed by the planning agent
$PA$.
\begin{table}[htbp]
  \centering\small
  \caption{Results for the MMDP, myopic, greedy
  algorithms.}\vspace{2pt}
  \begin{tabular}{l|c|c|c}
   & MMDP & myopic & greedy \\
  \hline
  \#completed tasks & 71\% & 65\% & 41\% \\
  \hline
  \#responders alive at the end & 100\% & 25\% & 0\% \\
  \end{tabular}
  \label{tab:simulation}\vspace{-3mm}
\end{table}
